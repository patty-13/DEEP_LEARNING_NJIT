{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patty-13/DEEP_LEARNING_NJIT/blob/main/HW3_DEEPLEARNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "# Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have\n",
        "- `data/train_split.txt`: training metadata\n",
        "- `data/train_labels`: training labels\n",
        "- `data/test_split.txt`: testing metadata\n",
        "- `data/feat/train/*.pt`: training feature\n",
        "- `data/feat/test/*.pt`:  testing feature\n",
        "\n",
        "after running the following block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkiMEcC3Foq",
        "outputId": "ac20b36c-9f75-4132-edc6-cec9299fec02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1rxGSKq18pO5HbHLx_mTk8NUHM96PxWFs \n",
            "\n",
            "unzip:  cannot find or open data.zip, data.zip.zip or data.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gdown\n",
        "\n",
        "# Main link\n",
        "!gdown --id '1rxGSKq18pO5HbHLx_mTk8NUHM96PxWFs' --output data.zip\n",
        "!unzip -q data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjO_P_uERwCd",
        "outputId": "ca250491-0e0c-4d52-c526-65ea3162220c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feat  test_split.txt  train_label.txt  train_split.txt\n"
          ]
        }
      ],
      "source": [
        "!ls data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pADUiYODJE1O"
      },
      "source": [
        "# Some Utility Functions\n",
        "**Fixes random number generator seeds for reproducibility.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsZKgBZQJjaE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def same_seeds(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJjLT8em-y9G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_feat(path):\n",
        "    feat = torch.load(path)\n",
        "    return feat\n",
        "\n",
        "def shift(x, n):\n",
        "    if n < 0:\n",
        "        left = x[0].repeat(-n, 1)\n",
        "        right = x[:n]\n",
        "    elif n > 0:\n",
        "        right = x[-1].repeat(n, 1)\n",
        "        left = x[n:]\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "    return torch.cat((left, right), dim=0)\n",
        "\n",
        "def concat_feat(x, concat_n):\n",
        "    assert concat_n % 2 == 1 # n must be odd\n",
        "    if concat_n < 2:\n",
        "        return x\n",
        "    seq_len, feature_dim = x.size(0), x.size(1)\n",
        "    x = x.repeat(1, concat_n)\n",
        "    x = x.view(seq_len, concat_n, feature_dim).permute(1, 0, 2) # concat_n, seq_len, feature_dim\n",
        "    mid = (concat_n // 2)\n",
        "    for r_idx in range(1, mid+1):\n",
        "        x[mid + r_idx, :] = shift(x[mid + r_idx], r_idx)\n",
        "        x[mid - r_idx, :] = shift(x[mid - r_idx], -r_idx)\n",
        "\n",
        "    return x.permute(1, 0, 2).view(seq_len, concat_n * feature_dim)\n",
        "\n",
        "def preprocess_data(split, feat_dir, phone_path, concat_nframes, train_ratio=0.8, random_seed=1213):\n",
        "    class_num = 41 # NOTE: pre-computed, should not need change\n",
        "\n",
        "    if split == 'train' or split == 'val':\n",
        "        mode = 'train'\n",
        "    elif split == 'test':\n",
        "        mode = 'test'\n",
        "    else:\n",
        "        raise ValueError('Invalid \\'split\\' argument for dataset: PhoneDataset!')\n",
        "\n",
        "    label_dict = {}\n",
        "    if mode == 'train':\n",
        "        for line in open(os.path.join(phone_path, f'{mode}_label.txt')).readlines():\n",
        "            line = line.strip('\\n').split(' ')\n",
        "            label_dict[line[0]] = [int(p) for p in line[1:]]\n",
        "\n",
        "        # split training and validation data\n",
        "        usage_list = open(os.path.join(phone_path, 'train_split.txt')).readlines()\n",
        "        random.seed(random_seed)\n",
        "        random.shuffle(usage_list)\n",
        "        train_len = int(len(usage_list) * train_ratio)\n",
        "        usage_list = usage_list[:train_len] if split == 'train' else usage_list[train_len:]\n",
        "\n",
        "    elif mode == 'test':\n",
        "        usage_list = open(os.path.join(phone_path, 'test_split.txt')).readlines()\n",
        "\n",
        "    usage_list = [line.strip('\\n') for line in usage_list]\n",
        "    print('[Dataset] - # phone classes: ' + str(class_num) + ', number of utterances for ' + split + ': ' + str(len(usage_list)))\n",
        "\n",
        "    max_len = 3000000\n",
        "    X = torch.empty(max_len, 39 * concat_nframes)\n",
        "    if mode == 'train':\n",
        "        y = torch.empty(max_len, dtype=torch.long)\n",
        "\n",
        "    idx = 0\n",
        "    for i, fname in tqdm(enumerate(usage_list)):\n",
        "        feat = load_feat(os.path.join(feat_dir, mode, f'{fname}.pt'))\n",
        "        cur_len = len(feat)\n",
        "        feat = concat_feat(feat, concat_nframes)\n",
        "        if mode == 'train':\n",
        "          label = torch.LongTensor(label_dict[fname])\n",
        "\n",
        "        X[idx: idx + cur_len, :] = feat\n",
        "        if mode == 'train':\n",
        "          y[idx: idx + cur_len] = label\n",
        "\n",
        "        idx += cur_len\n",
        "\n",
        "    X = X[:idx, :]\n",
        "    if mode == 'train':\n",
        "      y = y[:idx]\n",
        "\n",
        "    print(f'[INFO] {split} set')\n",
        "    print(X.shape)\n",
        "    if mode == 'train':\n",
        "      print(y.shape)\n",
        "      return X, y\n",
        "    else:\n",
        "      return X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = X\n",
        "        if y is not None:\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "# Model\n",
        "Feel free to modify the structure of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP5_-5DWYjwU"
      },
      "outputs": [],
      "source": [
        "#MODEL 1\n",
        "# Bidirectional LSTM (BiLSTM)\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "\n",
        "# class BiLSTM(nn.Module):\n",
        "#     def __init__(self, input_dim, num_layers, hidden_dim, num_classes):\n",
        "#         super(BiLSTM, self).__init__()\n",
        "#         self.hidden_dim = hidden_dim\n",
        "#         self.num_layers = num_layers\n",
        "#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "#         self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
        "#         c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
        "#         out, _ = self.lstm(x, (h0, c0))\n",
        "#         out = self.fc(out[:, -1, :])\n",
        "#         return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNKVnfgIY0M2"
      },
      "outputs": [],
      "source": [
        "# #MODEL 3\n",
        "# #TRANSFORMER\n",
        "# # import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.optim import Adam\n",
        "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# from torch.utils.data import DataLoader\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Define the CNN model\n",
        "# class CNNClassifier(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim, num_conv_layers=2, num_fc_layers=2, hidden_dim=512, dropout=0.5):\n",
        "#         super(CNNClassifier, self).__init__()\n",
        "\n",
        "#         self.conv_layers = nn.ModuleList()\n",
        "#         self.conv_layers.append(nn.Conv1d(400, hidden_dim, kernel_size=3, padding=1))\n",
        "#         self.conv_layers.append(nn.ReLU())\n",
        "#         self.conv_layers.append(nn.MaxPool1d(kernel_size=2))\n",
        "\n",
        "#         for _ in range(num_conv_layers - 1):\n",
        "#             self.conv_layers.append(nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1))\n",
        "#             self.conv_layers.append(nn.ReLU())\n",
        "#             self.conv_layers.append(nn.MaxPool1d(kernel_size=2))\n",
        "\n",
        "#         self.fc_layers = nn.ModuleList()\n",
        "#         self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "#         self.fc_layers.append(nn.ReLU())\n",
        "#         self.fc_layers.append(nn.Dropout(p=dropout))\n",
        "\n",
        "#         for _ in range(num_fc_layers - 1):\n",
        "#             self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "#             self.fc_layers.append(nn.ReLU())\n",
        "#             self.fc_layers.append(nn.Dropout(p=dropout))\n",
        "\n",
        "#         self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "#     # def forward(self, x):\n",
        "#     #     for layer in self.conv_layers:\n",
        "#     #         x = layer(x)\n",
        "\n",
        "#     #     x = x.mean(dim=2)  # Global average pooling\n",
        "\n",
        "#     #     for layer in self.fc_layers:\n",
        "#     #         x = layer(x)\n",
        "\n",
        "#     #     x = self.output_layer(x)\n",
        "#     #     return x\n",
        "#     def forward(self, x):\n",
        "\n",
        "#       for layer in self.conv_layers:\n",
        "#           x = layer(x)\n",
        "\n",
        "#       # Check the dimensions before applying global average pooling\n",
        "#       if x.dim() == 3:  # Ensure it's a 3D tensor (batch_size, channels, sequence_length)\n",
        "#           x = x.mean(dim=2)  # Global average pooling along the sequence_length dimension\n",
        "#       else:\n",
        "#           raise ValueError(\"Input tensor should be 3D with dimensions (batch_size, channels, sequence_length)\")\n",
        "\n",
        "#       for layer in self.fc_layers:\n",
        "#           x = layer(x)\n",
        "\n",
        "#       x = self.output_layer(x)\n",
        "#       return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg-GRd7ywdrL"
      },
      "outputs": [],
      "source": [
        "# MODEL 4\n",
        "# DEEP NEURAL NET\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_prob=0.1,bn_momentum=0.9, bn_eps=1e-5):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),\n",
        "            # nn.BatchNorm1d(output_dim),  # Batch normalization\n",
        "            nn.BatchNorm1d(output_dim, momentum=bn_momentum, eps=bn_eps),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_prob)  # Dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block(x)\n",
        "        return x\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=41, hidden_layers=3, hidden_dim=1024, dropout_prob=0.1):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        layers = [BasicBlock(input_dim, hidden_dim, dropout_prob)]\n",
        "        for _ in range(hidden_layers):\n",
        "            layers.append(BasicBlock(hidden_dim, hidden_dim, dropout_prob))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        self.fc = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# DEEP NEURAL NET WITH REGULARIZATION AND DIFFERENT ACTIVATION FUNCTION\n",
        "\n",
        "# import torch.nn as nn\n",
        "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# class BasicBlock(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim, dropout_prob=0.2, bn_momentum=0.9, bn_eps=1e-3):\n",
        "#         super(BasicBlock, self).__init__()\n",
        "\n",
        "#         self.block = nn.Sequential(\n",
        "#             nn.Linear(input_dim, output_dim),\n",
        "#             nn.BatchNorm1d(output_dim, momentum=bn_momentum, eps=bn_eps),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(p=dropout_prob)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.block(x)\n",
        "#         return x\n",
        "\n",
        "# class Classifier(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim=41, hidden_layers=1, hidden_dim=256, dropout_prob=0.2, l2_reg=0.001):\n",
        "#         super(Classifier, self).__init__()\n",
        "\n",
        "#         layers = [BasicBlock(input_dim, hidden_dim, dropout_prob)]\n",
        "#         for _ in range(hidden_layers):\n",
        "#             layers.append(BasicBlock(hidden_dim, hidden_dim, dropout_prob))\n",
        "\n",
        "#         layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "#         self.fc = nn.Sequential(*layers)\n",
        "#         self.l2_reg = l2_reg  # L2 regularization strength\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "#     def calculate_l2_regularization(self):\n",
        "#         l2_reg = 0.0\n",
        "#         for param in self.parameters():\n",
        "#             l2_reg += torch.norm(param, p=2)  # L2 norm of parameters\n",
        "#         return l2_reg\n",
        "\n",
        "#     def compute_loss(self, outputs, labels):\n",
        "#         l2_reg = self.calculate_l2_regularization()\n",
        "#         return nn.CrossEntropyLoss()(outputs, labels) + self.l2_reg * l2_reg\n",
        "\n",
        "\n",
        "# import torch.nn as nn\n",
        "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# class GatedReLU(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(GatedReLU, self).__init__()\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "#         self.relu = nn.ReLU()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.relu(x) * self.sigmoid(x)\n",
        "\n",
        "# class BasicBlock(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim, dropout_prob=0.2, bn_momentum=0.9, bn_eps=1e-3):\n",
        "#         super(BasicBlock, self).__init__()\n",
        "\n",
        "#         self.block = nn.Sequential(\n",
        "#             nn.Linear(input_dim, output_dim),\n",
        "#             nn.BatchNorm1d(output_dim, momentum=bn_momentum, eps=bn_eps),\n",
        "#             GatedReLU(),  # Use GatedReLU instead of ReLU\n",
        "#             nn.Dropout(p=dropout_prob)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.block(x)\n",
        "#         return x\n",
        "\n",
        "# class Classifier(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim=41, hidden_layers=1, hidden_dim=256, dropout_prob=0.2, l2_reg=0.001):\n",
        "#         super(Classifier, self).__init__()\n",
        "\n",
        "#         layers = [BasicBlock(input_dim, hidden_dim, dropout_prob)]\n",
        "#         for _ in range(hidden_layers):\n",
        "#             layers.append(BasicBlock(hidden_dim, hidden_dim, dropout_prob))\n",
        "\n",
        "#         # ReLU activation for the last hidden layer\n",
        "#         layers.append(nn.ReLU())\n",
        "\n",
        "#         # Softmax for the output layer\n",
        "#         layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "#         self.fc = nn.Sequential(*layers)\n",
        "#         self.l2_reg = l2_reg  # L2 regularization strength\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "#     def calculate_l2_regularization(self):\n",
        "#         l2_reg = 0.0\n",
        "#         for param in self.parameters():\n",
        "#             l2_reg += torch.norm(param, p=2)  # L2 norm of parameters\n",
        "#         return l2_reg\n",
        "\n",
        "#     def compute_loss(self, outputs, labels):\n",
        "#         l2_reg = self.calculate_l2_regularization()\n",
        "#         return nn.CrossEntropyLoss()(outputs, labels) + self.l2_reg * l2_reg\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlIq8JeqvvHC"
      },
      "source": [
        "# Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIHn79Iav1ri"
      },
      "outputs": [],
      "source": [
        "# data prarameters\n",
        "# TODO: change the value of \"concat_nframes\" for medium baseline\n",
        "concat_nframes = 41   # the number of frames to concat with, n must be odd (total 2k+1 = n frames)\n",
        "train_ratio = 0.80  # the ratio of data used for training, the rest will be used for validation\n",
        "\n",
        "# training parameters\n",
        "seed = 1213          # random seed\n",
        "batch_size = 300     # batch size\n",
        "num_epoch = 50         # the number of training epoch\n",
        "learning_rate = 1e-3     # learning rate\n",
        "model_path = './model.ckpt'  # the path where the checkpoint will be saved\n",
        "\n",
        "\n",
        "# model parameters\n",
        "# TODO: change the value of \"hidden_layers\" or \"hidden_dim\" for medium baseline\n",
        "input_dim = 39 * concat_nframes  # the input dim of the model, you should not change the value\n",
        "hidden_layers = 3      # the number of hidden layers\n",
        "hidden_dim = 1024       # the hidden dim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIUFRgG5yoDn"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1zI3v5jyrDn",
        "outputId": "de060048-43bf-4826-c592-f26c7956f419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cuda\n",
            "[Dataset] - # phone classes: 41, number of utterances for train: 2194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2194it [00:08, 264.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] train set\n",
            "torch.Size([1348721, 1599])\n",
            "torch.Size([1348721])\n",
            "[Dataset] - # phone classes: 41, number of utterances for val: 549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "549it [00:02, 238.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] val set\n",
            "torch.Size([348042, 1599])\n",
            "torch.Size([348042])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "\n",
        "same_seeds(seed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# preprocess data\n",
        "train_X, train_y = preprocess_data(split='train', feat_dir='./data/feat', phone_path='./data', concat_nframes=concat_nframes, train_ratio=train_ratio, random_seed=seed)\n",
        "val_X, val_y = preprocess_data(split='val', feat_dir='./data/feat', phone_path='./data', concat_nframes=concat_nframes, train_ratio=train_ratio, random_seed=seed)\n",
        "\n",
        "# get dataset\n",
        "train_set = SpeechDataset(train_X, train_y)\n",
        "val_set = SpeechDataset(val_X, val_y)\n",
        "\n",
        "# remove raw feature to save memory\n",
        "del train_X, train_y, val_X, val_y\n",
        "gc.collect()\n",
        "\n",
        "# get dataloader\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwWH1KIqzxEr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElNSQ_9tgMqC"
      },
      "outputs": [],
      "source": [
        "# import torch.nn.functional as F\n",
        "# class FocalLoss(nn.Module):\n",
        "#     def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "#         super(FocalLoss, self).__init__()\n",
        "#         self.alpha = alpha\n",
        "#         self.gamma = gamma\n",
        "#         self.reduction = reduction\n",
        "\n",
        "#     def forward(self, logits, targets):\n",
        "#         ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "#         pt = torch.exp(-ce_loss)\n",
        "#         focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "#         if self.reduction == 'mean':\n",
        "#             return torch.mean(focal_loss)\n",
        "#         elif self.reduction == 'sum':\n",
        "#             return torch.sum(focal_loss)\n",
        "#         else:\n",
        "#           return focal_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdMWsBs7zzNs",
        "outputId": "017d1917-002b-4476-fb64-84304eb16cbb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.91it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 257.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[001/050] Train Acc: 0.62228 Loss: 1.21785 | Val Acc: 0.66881 loss: 1.05046\n",
            "saving model with acc 0.66881\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 160.62it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 255.13it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[002/050] Train Acc: 0.70546 Loss: 0.92192 | Val Acc: 0.69205 loss: 0.98746\n",
            "saving model with acc 0.69205\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 161.70it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 255.43it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[003/050] Train Acc: 0.74809 Loss: 0.77451 | Val Acc: 0.70138 loss: 0.96670\n",
            "saving model with acc 0.70138\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.70it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 257.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[004/050] Train Acc: 0.78087 Loss: 0.66258 | Val Acc: 0.70387 loss: 0.99223\n",
            "saving model with acc 0.70387\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.79it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 255.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[005/050] Train Acc: 0.80568 Loss: 0.57796 | Val Acc: 0.70387 loss: 1.01790\n",
            "saving model with acc 0.70387\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.94it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 255.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[006/050] Train Acc: 0.82604 Loss: 0.51218 | Val Acc: 0.70451 loss: 1.08152\n",
            "saving model with acc 0.70451\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.70it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 257.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[007/050] Train Acc: 0.84181 Loss: 0.46150 | Val Acc: 0.70087 loss: 1.10423\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.34it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 253.07it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[008/050] Train Acc: 0.85376 Loss: 0.42397 | Val Acc: 0.70232 loss: 1.13749\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.87it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 252.83it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[009/050] Train Acc: 0.86369 Loss: 0.39377 | Val Acc: 0.70205 loss: 1.15867\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.41it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 253.08it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
            "[010/050] Train Acc: 0.87188 Loss: 0.36902 | Val Acc: 0.69901 loss: 1.18642\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.59it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 254.24it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[011/050] Train Acc: 0.90421 Loss: 0.27283 | Val Acc: 0.71043 loss: 1.28384\n",
            "saving model with acc 0.71043\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.63it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 256.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[012/050] Train Acc: 0.91661 Loss: 0.23708 | Val Acc: 0.71143 loss: 1.28029\n",
            "saving model with acc 0.71143\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.84it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 252.56it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[013/050] Train Acc: 0.92179 Loss: 0.22118 | Val Acc: 0.71160 loss: 1.27459\n",
            "saving model with acc 0.71160\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.89it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 252.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[014/050] Train Acc: 0.92520 Loss: 0.21080 | Val Acc: 0.70660 loss: 1.36218\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.41it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 251.08it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[015/050] Train Acc: 0.92800 Loss: 0.20354 | Val Acc: 0.70995 loss: 1.31756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.41it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 250.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[016/050] Train Acc: 0.93024 Loss: 0.19717 | Val Acc: 0.70951 loss: 1.40253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 164.46it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 250.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[017/050] Train Acc: 0.93210 Loss: 0.19168 | Val Acc: 0.71185 loss: 1.34984\n",
            "saving model with acc 0.71185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.86it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 248.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[018/050] Train Acc: 0.93337 Loss: 0.18734 | Val Acc: 0.70986 loss: 1.39895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 165.01it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 249.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[019/050] Train Acc: 0.93534 Loss: 0.18290 | Val Acc: 0.70785 loss: 1.39735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 164.75it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 246.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[020/050] Train Acc: 0.93627 Loss: 0.17899 | Val Acc: 0.70883 loss: 1.40578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.62it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 251.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00021: reducing learning rate of group 0 to 1.0000e-05.\n",
            "[021/050] Train Acc: 0.93757 Loss: 0.17534 | Val Acc: 0.71005 loss: 1.41124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.04it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 248.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[022/050] Train Acc: 0.94043 Loss: 0.16751 | Val Acc: 0.70931 loss: 1.40907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 164.40it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 250.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[023/050] Train Acc: 0.94130 Loss: 0.16490 | Val Acc: 0.71133 loss: 1.40363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.03it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 249.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[024/050] Train Acc: 0.94180 Loss: 0.16389 | Val Acc: 0.71018 loss: 1.41859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.96it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 251.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00025: reducing learning rate of group 0 to 1.0000e-06.\n",
            "[025/050] Train Acc: 0.94166 Loss: 0.16384 | Val Acc: 0.71127 loss: 1.42805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 164.11it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 251.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[026/050] Train Acc: 0.94209 Loss: 0.16225 | Val Acc: 0.71040 loss: 1.45997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.35it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 253.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[027/050] Train Acc: 0.94210 Loss: 0.16269 | Val Acc: 0.71193 loss: 1.43168\n",
            "saving model with acc 0.71193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.70it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 255.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[028/050] Train Acc: 0.94213 Loss: 0.16249 | Val Acc: 0.70915 loss: 1.42216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.12it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 255.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[029/050] Train Acc: 0.94243 Loss: 0.16169 | Val Acc: 0.70806 loss: 1.49911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.28it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 255.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[030/050] Train Acc: 0.94242 Loss: 0.16132 | Val Acc: 0.71091 loss: 1.43654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.45it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 251.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00031: reducing learning rate of group 0 to 1.0000e-07.\n",
            "[031/050] Train Acc: 0.94257 Loss: 0.16177 | Val Acc: 0.70979 loss: 1.44783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.79it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 253.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[032/050] Train Acc: 0.94249 Loss: 0.16169 | Val Acc: 0.70999 loss: 1.44071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.55it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 253.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[033/050] Train Acc: 0.94235 Loss: 0.16151 | Val Acc: 0.71021 loss: 1.44052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 164.90it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 251.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[034/050] Train Acc: 0.94239 Loss: 0.16172 | Val Acc: 0.70920 loss: 1.45064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.48it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 249.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00035: reducing learning rate of group 0 to 1.0000e-08.\n",
            "[035/050] Train Acc: 0.94272 Loss: 0.16118 | Val Acc: 0.71055 loss: 1.41796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 164.11it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 247.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[036/050] Train Acc: 0.94256 Loss: 0.16176 | Val Acc: 0.70953 loss: 1.42987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.71it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 250.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[037/050] Train Acc: 0.94241 Loss: 0.16178 | Val Acc: 0.71247 loss: 1.39810\n",
            "saving model with acc 0.71247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.77it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 249.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[038/050] Train Acc: 0.94276 Loss: 0.16152 | Val Acc: 0.70947 loss: 1.41546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 164.02it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 249.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[039/050] Train Acc: 0.94257 Loss: 0.16136 | Val Acc: 0.71043 loss: 1.43289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.16it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 249.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[040/050] Train Acc: 0.94253 Loss: 0.16117 | Val Acc: 0.71002 loss: 1.41384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.81it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 252.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[041/050] Train Acc: 0.94240 Loss: 0.16154 | Val Acc: 0.71117 loss: 1.40389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.94it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 251.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[042/050] Train Acc: 0.94247 Loss: 0.16196 | Val Acc: 0.70953 loss: 1.45937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 164.01it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 250.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[043/050] Train Acc: 0.94292 Loss: 0.16084 | Val Acc: 0.71132 loss: 1.43154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 162.82it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 248.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[044/050] Train Acc: 0.94266 Loss: 0.16151 | Val Acc: 0.70975 loss: 1.45121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.74it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 249.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[045/050] Train Acc: 0.94246 Loss: 0.16163 | Val Acc: 0.71127 loss: 1.46887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 164.59it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 248.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[046/050] Train Acc: 0.94219 Loss: 0.16230 | Val Acc: 0.71091 loss: 1.43767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 165.21it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 255.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[047/050] Train Acc: 0.94248 Loss: 0.16191 | Val Acc: 0.70925 loss: 1.45135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 164.08it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 253.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[048/050] Train Acc: 0.94287 Loss: 0.16112 | Val Acc: 0.70887 loss: 1.46228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.60it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 254.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[049/050] Train Acc: 0.94263 Loss: 0.16111 | Val Acc: 0.71145 loss: 1.41722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4496/4496 [00:27<00:00, 163.78it/s]\n",
            "100%|██████████| 1161/1161 [00:04<00:00, 255.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[050/050] Train Acc: 0.94267 Loss: 0.16148 | Val Acc: 0.71187 loss: 1.44660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# create model, define a loss function, and optimizer\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "#model = CNNClassifier(input_dim=input_dim, output_dim=41, num_conv_layers=num_conv_layers,\n",
        "#                      num_fc_layers=num_fc_layers, hidden_dim=hidden_dim, dropout=0.5).to(device)\n",
        "\n",
        "model = Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim).to(device)\n",
        "#model = BiLSTM(input_dim=input_dim, num_layers=hidden_layers, hidden_dim=hidden_dim).to(device)\n",
        "#model = TransformerClassifier(input_dim=input_dim, num_layers=num_layers, hidden_dim=hidden_dim,\n",
        "#                              output_dim=41, nhead=nhead).to(device)\n",
        "#criterion = FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.1, verbose=True)\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, batch in enumerate(tqdm(train_loader)):\n",
        "        features, labels = batch\n",
        "        features = features.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        train_acc += (train_pred.detach() == labels.detach()).sum().item()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # validation\n",
        "    model.eval() # set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(val_loader)):\n",
        "            features, labels = batch\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(features)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, val_pred = torch.max(outputs, 1)\n",
        "            val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "    print(f'[{epoch+1:03d}/{num_epoch:03d}] Train Acc: {train_acc/len(train_set):3.5f} Loss: {train_loss/len(train_loader):3.5f} | Val Acc: {val_acc/len(val_set):3.5f} loss: {val_loss/len(val_loader):3.5f}')\n",
        "\n",
        "    # if the model improves, save a checkpoint at this epoch\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f'saving model with acc {best_acc/len(val_set):.5f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab33MxosWLmG",
        "outputId": "8f10b8aa-6abb-468f-b2e5-8188274d0d90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "del train_set, val_set\n",
        "del train_loader, val_loader\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "# Testing\n",
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOG1Ou0PGrhc",
        "outputId": "ed47ef2b-f989-4801-d1fe-294260207ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dataset] - # phone classes: 41, number of utterances for test: 686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "686it [00:02, 278.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] test set\n",
            "torch.Size([420031, 1599])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "test_X = preprocess_data(split='test', feat_dir='./data/feat', phone_path='./data', concat_nframes=concat_nframes)\n",
        "test_set = SpeechDataset(test_X, None)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay0Fu8Ovkdad",
        "outputId": "f5826c35-37c2-4bb9-ba6a-c86587c1eae6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# load model\n",
        "model = Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim).to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp-DV1p4r7Nz"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84HU5GGjPqR0",
        "outputId": "0f88f2b3-c196-4e71-9b01-c8bf296387b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1401/1401 [00:04<00:00, 342.68it/s]\n"
          ]
        }
      ],
      "source": [
        "pred = np.array([], dtype=np.int32)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(tqdm(test_loader)):\n",
        "        features = batch\n",
        "        features = features.to(device)\n",
        "\n",
        "        outputs = model(features)\n",
        "\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        pred = np.concatenate((pred, test_pred.cpu().numpy()), axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyZqy40Prz0v"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "outputs": [],
      "source": [
        "with open('predictiontrial_94267train_71187_vald_acc.csv', 'w') as f:\n",
        "    f.write('Id,Label\\n')\n",
        "    for i, y in enumerate(pred):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "C9RKyh_JNomf",
        "outputId": "ea23b665-6b41-4b77-876a-d05743071d0f"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-55abe0b0821d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/prediction.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/prediction.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('file.txt', sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm16qBZqMUBw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}